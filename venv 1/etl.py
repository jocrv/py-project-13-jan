etl
extract, transform, load // microsoft a-ure
ingest data lake,  data factory resources group
subscriptio pay as you go 
east us 2 region (cheaper)
prepare
data factory, author (data sets, pipeline)
pipeline runs (copy to lake)

// you can use in the space github or devops
// author: integration time from data factory, linked services with the data store, you can use the more than 80 resources or services (git, github, api hash, etc... data stores, hdfs, on...)
teste connection and create with the connections with the origin, extract this data and give to data lake,//
data lake could given to another data repository, but you have to connections between that. 
3 part: data section, data flow,
engenharia de vari√°veis 